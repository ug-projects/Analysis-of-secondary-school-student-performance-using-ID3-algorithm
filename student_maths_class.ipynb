{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datamat = pd.read_csv('student-mat.csv')\n",
    "datamat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datamat.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datamat.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating class labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datamat = pd.read_csv('student-mat.csv')\n",
    "\n",
    "data1 = datamat\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "lb_make = LabelEncoder()\n",
    "for i in data1.columns:\n",
    "    data1[i] = lb_make.fit_transform(data1[i])\n",
    "ng = []\n",
    "for i in data1['G3']:\n",
    "    if (i>=0 and i<=8):\n",
    "        ng.append(0)\n",
    "    elif (i>=9 and i<=12):\n",
    "        ng.append(1)\n",
    "    else:\n",
    "        ng.append(2)\n",
    "\n",
    "data1['G3'] = ng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.countplot(data1['G3'],label=\"Grade in Maths class\")\n",
    "plt.title('Grade in Maths class')\n",
    "plt.xticks(fontsize=14, rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(tc):\n",
    "    elements,counts = np.unique(tc,return_counts = True)\n",
    "    entropy = np.sum([(-counts[i]/np.sum(counts))*np.log2(counts[i]/np.sum(counts)) for i in range(len(elements))])\n",
    "    return entropy\n",
    "\n",
    "def InfoGain(data,split_attribute_name,target_name=\"G3\"):\n",
    "    total_entropy = entropy(data[target_name]) \n",
    "    vals,counts= np.unique(data[split_attribute_name],return_counts=True)\n",
    "    Weighted_Entropy = np.sum([(counts[i]/np.sum(counts))*entropy(data.where(data[split_attribute_name]==vals[i]).dropna()[target_name]) for i in range(len(vals))])\n",
    "    Information_Gain = total_entropy - Weighted_Entropy\n",
    "    return Information_Gain\n",
    "\n",
    "\n",
    "def ID3(data,originaldata,features,target_attribute_name=\"G3\",parent_node_class = None):\n",
    "    if len(np.unique(data[target_attribute_name])) <= 1:\n",
    "        return np.unique(data[target_attribute_name])[0]\n",
    "    elif len(data)==0:\n",
    "        return np.unique(originaldata[target_attribute_name])[np.argmax(np.unique(originaldata[target_attribute_name],return_counts=True)[1])]\n",
    "\n",
    "    elif len(features) ==0:\n",
    "        return parent_node_class\n",
    "\n",
    "    else:\n",
    "        parent_node_class = np.unique(data[target_attribute_name])[np.argmax(np.unique(data[target_attribute_name],return_counts=True)[1])]\n",
    "        \n",
    "        item_values = [InfoGain(data,feature,target_attribute_name) for feature in features]\n",
    "        best_feature_index = np.argmax(item_values)\n",
    "        best_feature = features[best_feature_index]\n",
    "\n",
    "        tree = {best_feature:{}}\n",
    "        \n",
    "        \n",
    "        features = [i for i in features if i != best_feature]\n",
    "        \n",
    "        \n",
    "        for value in np.unique(data[best_feature]):\n",
    "            value = value\n",
    "            sub_data = data.where(data[best_feature] == value).dropna()\n",
    "            subtree = ID3(sub_data,dataset,features,target_attribute_name,parent_node_class)\n",
    "            tree[best_feature][value] = subtree\n",
    "            \n",
    "        return(tree)    \n",
    "\n",
    "    \n",
    "def predictt(query,tree,default = 1):\n",
    "    for key in list(query.keys()):\n",
    "        if key in list(tree.keys()):\n",
    "            try:\n",
    "                result = tree[key][query[key]] \n",
    "            except:\n",
    "                return default\n",
    "            result = tree[key][query[key]]\n",
    "            if isinstance(result,dict):\n",
    "                return predictt(query,result)\n",
    "            else:\n",
    "                return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling the data for train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "def train_test_split(dataset):\n",
    "    train_data = resample(data1, replace=True, n_samples=int(2*data1.shape[0]/3), random_state=1).reset_index(drop=True)\n",
    "    test_data = resample(data1, replace=True, n_samples=int(1*data1.shape[0]/3), random_state=2).reset_index(drop=True)\n",
    "    return train_data,test_data\n",
    "\n",
    "dataset = data1\n",
    "training_data = train_test_split(dataset)[0]\n",
    "testing_data = train_test_split(dataset)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = ID3(training_data,training_data,training_data.columns[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(data,tree):\n",
    "    queries = data.iloc[:,:-1].to_dict(orient = \"records\")\n",
    "    predicted = pd.DataFrame(columns=[\"predicted\"]) \n",
    "    for i in range(len(data)):\n",
    "        predicted.loc[i,\"predicted\"] = predictt(queries[i],tree,1.0) \n",
    "    print('The prediction accuracy is: ',(np.sum(predicted[\"predicted\"] == data[\"G3\"])/len(data))*100,'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(testing_data,tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## kFold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randrange\n",
    "def cross_validation_split(dataset, folds):\n",
    "        dataset_split = []\n",
    "        df_copy = dataset\n",
    "        fold_size = int(df_copy.shape[0] / folds)\n",
    "        \n",
    "        for i in range(folds):\n",
    "            fold = []\n",
    "            while len(fold) < fold_size:\n",
    "                r = randrange(df_copy.shape[0])\n",
    "                index = df_copy.index[r]\n",
    "                fold.append(df_copy.loc[index].values.tolist())\n",
    "                df_copy = df_copy.drop(index)\n",
    "            dataset_split.append(np.asarray(fold))\n",
    "        return dataset_split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testkfold(data,tree):\n",
    "    queries = data.iloc[:,:-1].to_dict(orient = \"records\")\n",
    "    predicted = pd.DataFrame(columns=[\"predicted\"]) \n",
    "    for i in range(len(data)):\n",
    "        predicted.loc[i,\"predicted\"] = predictt(queries[i],tree,1.0) \n",
    "    return ((np.sum(predicted[\"predicted\"] == data[\"G3\"])/len(data))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kfoldCV(dataset, f, k):\n",
    "    data=cross_validation_split(dataset,f)\n",
    "    result=[]\n",
    "    for i in range(f):\n",
    "        r = list(range(f))\n",
    "        r.pop(i)\n",
    "        for j in r :\n",
    "            if j == r[0]:\n",
    "                cv = data[j]\n",
    "            else:    \n",
    "                cv=np.concatenate((cv,data[j]), axis=0)\n",
    "        dtf = pd.DataFrame(cv)\n",
    "        dtf.columns = data1.columns\n",
    "        if True:\n",
    "            acc = testkfold(dtf,tree)\n",
    "        result.append(acc)\n",
    "    return np.mean(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valcatch = []\n",
    "for i in range(1,10):\n",
    "    v = kfoldCV(testing_data,10,i)\n",
    "    print(v)\n",
    "    valcatch.append(v)\n",
    "plt.plot(valcatch)\n",
    "plt.title('KFold cross validation with different k values')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_y(data,tree):\n",
    "    queries = data.iloc[:,:-1].to_dict(orient = \"records\")\n",
    "    predicted = pd.DataFrame(columns=[\"predicted\"]) \n",
    "    for i in range(len(data)):\n",
    "        predicted.loc[i,\"predicted\"] = predictt(queries[i],tree,1.0) \n",
    "    return list(predicted.predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = testing_data.iloc[:,-1]\n",
    "y_pred = predict_y(testing_data,tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "currentDataClass = y_true\n",
    "predictedClass = y_pred\n",
    "classes = int(max(currentDataClass) - min(currentDataClass)) + 1\n",
    "\n",
    "counts = [[sum([(currentDataClass[i] == true_class) and (predictedClass[i] == pred_class) \n",
    "                for i in range(len(currentDataClass))])\n",
    "           for pred_class in range(0, classes )] \n",
    "           for true_class in range(0, classes )]\n",
    "counts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_mat = pd.DataFrame(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot_confusion_matrix(df_confusion, title='Confusion matrix', cmap=plt.cm.gray_r):\n",
    "    plt.matshow(df_confusion, cmap=cmap)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(df_confusion.columns))\n",
    "    plt.xticks(tick_marks, df_confusion.columns, rotation=45)\n",
    "    plt.yticks(tick_marks, df_confusion.index)\n",
    "    plt.ylabel(df_confusion.index.name)\n",
    "    plt.xlabel(df_confusion.columns.name)\n",
    "\n",
    "plot_confusion_matrix(conf_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_1 = conf_mat[0][0]/(conf_mat[0][0]+conf_mat[1][0]+conf_mat[2][0])\n",
    "precision_2 = conf_mat[1][1]/(conf_mat[0][1]+conf_mat[1][1]+conf_mat[2][1])\n",
    "precision_3 = conf_mat[2][2]/(conf_mat[0][2]+conf_mat[1][2]+conf_mat[2][2])\n",
    "recall_1 = conf_mat[0][0]/(conf_mat[0][0]+conf_mat[0][1]+conf_mat[0][2])\n",
    "recall_2 = conf_mat[1][1]/(conf_mat[1][0]+conf_mat[1][1]+conf_mat[1][2])\n",
    "recall_3 = conf_mat[2][2]/(conf_mat[2][0]+conf_mat[2][1]+conf_mat[2][2])\n",
    "f1_score_1 = 2 * precision_1 * recall_1 / (precision_1 + recall_1)\n",
    "f1_score_2 = 2 * precision_2 * recall_2 / (precision_2 + recall_2)\n",
    "f1_score_3 = 2 * precision_3 * recall_3 / (precision_3 + recall_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
